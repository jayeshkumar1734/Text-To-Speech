{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOlFSoEE8spR",
    "outputId": "ec960306-65de-4b28-f8a1-981893761ae2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayesh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jayesh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jayesh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jayesh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jayesh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jayesh/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental.preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a697c8135770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     raise ImportError(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device_gpu)\n",
    "from sklearn.model_selection import train_test_split \n",
    "import os,glob\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import librosa\n",
    "import librosa.display\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "import os,glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.figure_factory as ff\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DOt-4UNs81rl"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model,inference, max_seq_len=3000):\n",
    "        super(PositionalEncoder,self).__init__()\n",
    "        #print(max_seq_len)\n",
    "        self.inference=inference\n",
    "        self.d_model = d_model\n",
    "        self.alpha_factor=nn.Parameter(torch.diag(torch.ones(max_seq_len)))\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] =math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] =math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #scaling\n",
    "        self.pe=torch.matmul(self.alpha_factor,self.pe)\n",
    "        #print(self.pe.shape, x.shape)\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #print(x.shape)\n",
    "        #add constant to embedding\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        # print(x.shape)\n",
    "        #print(\"ll\",seq_len)\n",
    "        # x=x.to(device_gpu)\n",
    "        if self.inference == False:\n",
    "          x = x + Variable(self.pe[:,:seq_len], requires_grad=False).cuda()\n",
    "        else:\n",
    "          x = x + Variable(self.pe[:,:seq_len], requires_grad=False).to('cpu')\n",
    "        #print(\"hg\",x.shape, self.pe[:,:seq_len].shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7GP6FH2rrQu3"
   },
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self,  d_model, max_seq_len=200):\n",
    "      super().__init__()\n",
    "      self.embed = nn.Embedding(max_seq_len, d_model,padding_idx=0)\n",
    "    def forward(self, x):\n",
    "      return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yVJJkhvUrnw7"
   },
   "outputs": [],
   "source": [
    "class Encoder_prenet(nn.Module):\n",
    "  def __init__(self,d_model,max_seq_len):\n",
    "    super(Encoder_prenet,self).__init__()\n",
    "    convolutions=[]\n",
    "    for _ in range(3):\n",
    "      conv=nn.Sequential(\n",
    "          nn.Conv1d(in_channels=d_model,out_channels=d_model,kernel_size=5,padding=2),\n",
    "          nn.BatchNorm1d(d_model),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.1)\n",
    "      )\n",
    "      convolutions.append(conv)\n",
    "    self.convolutions=nn.ModuleList(convolutions)\n",
    "    self.projection=nn.Linear(max_seq_len,max_seq_len)\n",
    "  def forward(self,x):\n",
    "    # print(x.shape)\n",
    "    x=x.reshape(x.shape[0],x.shape[2],x.shape[1])\n",
    "    for conv in self.convolutions:\n",
    "      x=conv(x)\n",
    "      \n",
    "    # x = x.view(x.size(0), -1)\n",
    "    \n",
    "    x=self.projection(x)\n",
    "    x=x.transpose(1,2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WTbzyCh0JQYa"
   },
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "  def __init__ (self,heads,d_model,inference=False,dropout=0.1):\n",
    "    super(Multi_Head_Attention,self).__init__()\n",
    "    self.inference=inference\n",
    "    self.heads=heads\n",
    "    self.d_k=d_model//heads\n",
    "    self.q_linear=nn.Linear(d_model,d_model)\n",
    "    self.v_linear=nn.Linear(d_model,d_model)\n",
    "    self.k_linear=nn.Linear(d_model,d_model)\n",
    "    self.out = nn.Linear(d_model, d_model)\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "    self.d_model=d_model\n",
    "\n",
    "  def forward(self,q,k,v,mask=None):\n",
    "   \n",
    "    bs_q=q.size(0)\n",
    "    bs_v=v.size(0)\n",
    "    bs_k=k.size(0)\n",
    "    q=q.type(torch.FloatTensor)\n",
    "    v=v.type(torch.FloatTensor)\n",
    "    k=k.type(torch.FloatTensor)\n",
    "    \n",
    "    if self.inference == False:\n",
    "      q=q.to(device_gpu)\n",
    "      v=v.to(device_gpu)\n",
    "      k=k.to(device_gpu)\n",
    "    \n",
    "    q=self.q_linear(q).view(bs_q,-1,self.heads,self.d_k)\n",
    "    v=self.v_linear(v).view(bs_v,-1,self.heads,self.d_k)\n",
    "    k=self.k_linear(k).view(bs_k,-1,self.heads,self.d_k)\n",
    "    q=q.transpose(1,2)\n",
    "    v=v.transpose(1,2)\n",
    "    k=k.transpose(1,2)\n",
    "    # calculate attention using function we will define next\n",
    "    scores = attention(q=q, k=k, v=v, d_k=self.d_k, inference=self.inference,mask=mask, dropout=self.dropout)\n",
    "    \n",
    "    # concatenate heads and put through final linear layer\n",
    "    concat = scores.transpose(1,2).contiguous().view(bs_q, -1, self.d_model)\n",
    "    output = self.out(concat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9kc8MNAkV2sr"
   },
   "outputs": [],
   "source": [
    "  def attention(q, k, v, d_k, inference=False,mask=None, dropout=None):\n",
    "    #print(q.shape,k.transpose(-2,-1).shape)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "      size=scores.size(2)\n",
    "      #print(size)\n",
    "      np_mask = np.triu(np.ones((1, size, size),dtype='uint8'),k=1)\n",
    "      np_mask=torch.FloatTensor(np_mask)\n",
    "      #print(np_mask)\n",
    "      np_mask = np_mask.unsqueeze(1)\n",
    "      if inference==False:\n",
    "        np_mask=np_mask.to(device_gpu)\n",
    "      scores = scores.masked_fill(np_mask == 1, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1,dtype=torch.float)\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wjBXqg41JSNy"
   },
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super(Norm,self).__init__()\n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        # self.apha , self.bias , self.eps = self.alpha.to(device_gpu) , self.bias.to(device_gpu), self.eps.to(device_gpu) \n",
    "        # x=x.to('cpu')\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KY2nSp1aJZ9s"
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "    super(FFN,self).__init__() \n",
    "    # We set d_ff as a default to 2048\n",
    "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "  def forward(self, x):\n",
    "    x = self.dropout(F.relu(self.linear_1(x)))\n",
    "    x = self.linear_2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvgF_EUSJckV"
   },
   "outputs": [],
   "source": [
    "class Encoder_layer(nn.Module):\n",
    "  def __init__(self,d_model,heads,inference,dropout=0.1):\n",
    "    super(Encoder_layer,self).__init__()\n",
    "    self.norm_1=Norm(d_model)\n",
    "    self.norm_2=Norm(d_model)\n",
    "    self.ffn=FFN(d_model)\n",
    "    self.attn=Multi_Head_Attention(heads,d_model,inference)\n",
    "    self.dropout_1=nn.Dropout(dropout)\n",
    "    self.dropout_2=nn.Dropout(dropout)\n",
    "  def forward(self,x):\n",
    "    # x=x.to('cpu')\n",
    "    \n",
    "    x2=self.norm_1(x)\n",
    "   \n",
    "    x=x+self.dropout_1(self.attn(x2,x2,x2))\n",
    "    x2=self.norm_2(x)\n",
    "    x=x+self.dropout_2(self.ffn(x2))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6n6fTGZjJhhB"
   },
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QK7cDpHMJvh0"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,d_model,heads,max_seq_len,inference,N):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.N=N\n",
    "    self.embed=Embedder(d_model)\n",
    "    self.prenet=Encoder_prenet(d_model,max_seq_len)\n",
    "    self.pe=PositionalEncoder(d_model,inference)\n",
    "    self.layers=get_clones(Encoder_layer(d_model,heads,inference),N)\n",
    "    self.norm=Norm(d_model)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x=self.embed(x)\n",
    "    x=self.prenet(x)\n",
    "    x=self.pe(x)\n",
    "    for i in range(self.N):\n",
    "      x=self.layers[i](x)\n",
    "    x=self.norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nP64SJhOVSpm"
   },
   "outputs": [],
   "source": [
    "class Decoder_prenet(nn.Module):\n",
    "  def __init__(self,d_model,dropout=0.1):\n",
    "    super(Decoder_prenet,self).__init__()\n",
    "    self.linear_1=nn.Linear(80,256)\n",
    "    self.linear_2=nn.Linear(256,256)\n",
    "    self.linear_3=nn.Linear(256,d_model)\n",
    "    self.dropout_1=nn.Dropout(dropout)\n",
    "    self.dropout_2=nn.Dropout(dropout)\n",
    "  def forward(self,x):\n",
    "    \n",
    "\n",
    "    x=self.dropout_1(F.relu(self.linear_1(x)))\n",
    "    #print(x.shape)\n",
    "    x=self.dropout_2(F.relu(self.linear_2(x)))\n",
    "    x=self.linear_3(x)\n",
    "    # x=x.unsqueeze(0)\n",
    "    # x=x.transpose(1,2)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDFj5aBHqnBe"
   },
   "outputs": [],
   "source": [
    "class Decoder_Layer(nn.Module):\n",
    "  def __init__(self,d_model,heads,inference,dropout=0.1):\n",
    "    super(Decoder_Layer,self).__init__()\n",
    "    self.norm_1=Norm(d_model)\n",
    "    self.norm_2=Norm(d_model)\n",
    "    self.norm_3=Norm(d_model)\n",
    "    self.att_1=Multi_Head_Attention(heads,d_model,inference)\n",
    "    self.att_2=Multi_Head_Attention(heads,d_model,inference)\n",
    "    self.ffn=FFN(d_model)\n",
    "    self.dropout_1=nn.Dropout(dropout)\n",
    "    self.dropout_2=nn.Dropout(dropout)\n",
    "    self.dropout_3=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self,x,e_output,mask=1):\n",
    "    # x=x.to('cpu')    \n",
    "    x2=self.norm_1(x)\n",
    "    \n",
    "    \n",
    "    x=x+self.dropout_1(self.att_1(x2,x2,x2,mask))\n",
    "    x2=self.norm_1(x)\n",
    "    \n",
    "    x=x+self.dropout_2(self.att_2(x2,e_output,e_output))\n",
    "    \n",
    "    x2=self.norm_3(x)\n",
    "    x=x+self.dropout_3(self.ffn(x2))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8So5jD8qsCz"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,d_model,heads,inference,N):\n",
    "    super(Decoder,self).__init__()\n",
    "    self.N=N\n",
    "    self.pe=PositionalEncoder(d_model,inference)\n",
    "    self.prenet=Decoder_prenet(d_model)\n",
    "    self.layers=get_clones(Decoder_Layer(d_model,heads,inference),N)\n",
    "    self.norm=Norm(d_model)\n",
    "  def forward(self,x,e_output,temp=1):\n",
    "    x=self.prenet(x)\n",
    "\n",
    "    x=self.pe(x)\n",
    "    for i in range(self.N):\n",
    "      x=self.layers[i](x,e_output)\n",
    "    # print(x.shape)\n",
    "    return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNHtcuxQqwVn",
    "outputId": "9965afec-2d37-4cd4-e847-76f1b74b7737"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1639])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1=pd.read_pickle(\"/content/drive/MyDrive/Text-To-Speech_HINDI/chosen_files.pkl\")\n",
    "df_1[\"mel_output\"].values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PeEuav_YAyk"
   },
   "outputs": [],
   "source": [
    "class Linear_Projection(nn.Module):\n",
    "  def __init__(self,d_model,channels):\n",
    "    super(Linear_Projection,self).__init__()\n",
    "    self.linear=nn.Linear(d_model,channels)\n",
    "    torch.nn.init.xavier_normal_(self.linear.weight)\n",
    "  def forward(self,x):\n",
    "    # x=x.type(torch.LongTensor)\n",
    "    return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6-r5_9gRolW"
   },
   "outputs": [],
   "source": [
    "class Conv_layer(nn.Module):\n",
    "  def __init__(self,input,output,kernel,stride,pad,dilation,w_init_gain=\"linear\"):\n",
    "    super(Conv_layer,self).__init__()\n",
    "    if pad is None:\n",
    "      assert(kernel % 2 == 1)\n",
    "      pad = int(dilation * (kernel - 1) / 2)\n",
    "    self.cnn_layer=nn.Conv1d(in_channels=input,out_channels=output,kernel_size=kernel,stride=stride,padding=pad,dilation=dilation)\n",
    "    torch.nn.init.xavier_uniform(self.cnn_layer.weight,gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "  def forward(self,x):\n",
    "    # x=x.type(torch.LongTensor)\n",
    "    return self.cnn_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Qxv9crXRsRZ"
   },
   "outputs": [],
   "source": [
    "class CNN_projection(nn.Module):\n",
    "  def __init__(self,channels,output,kernel,dropout=0.1):\n",
    "    super(CNN_projection,self).__init__()\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "    self.cnn_model=nn.ModuleList()\n",
    "    self.cnn_model.append(\n",
    "        nn.Sequential(Conv_layer(channels,output,kernel,1,int((kernel-1)/2),1,\"tanh\"),\n",
    "                      nn.BatchNorm1d(output))\n",
    "    )\n",
    "    for i in range(1,4):\n",
    "      self.cnn_model.append(\n",
    "          nn.Sequential(Conv_layer(output,output,kernel,1,int((kernel-1)/2),1,\"tanh\"),\n",
    "                      nn.BatchNorm1d(output))\n",
    "    )\n",
    "    \n",
    "    self.cnn_model.append(\n",
    "        nn.Sequential(Conv_layer(output,channels,kernel,1,int((kernel-1)/2),1,\"linear\"),\n",
    "                      nn.BatchNorm1d(channels))\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    for i in range(len(self.cnn_model)-1):\n",
    "      x=self.dropout(torch.tanh(self.cnn_model[i](x)))\n",
    "    x=self.dropout(self.cnn_model[-1](x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smWwGlW6RuNg"
   },
   "outputs": [],
   "source": [
    "class Postnet(nn.Module):\n",
    "  def __init__(self,d_model,channels_mel,channels_stop,kernel,output):\n",
    "    super(Postnet,self).__init__()\n",
    "    self.mel_linear=Linear_Projection(d_model,channels_mel)\n",
    "    self.stop_linear=Linear_Projection(d_model,channels_stop)\n",
    "    self.layers_cnn=CNN_projection(channels_mel,output,kernel)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    \n",
    "    x1=self.stop_linear(x)\n",
    "    x2=self.mel_linear(x)\n",
    "    x2=x2.transpose(1,2)\n",
    "    x3=self.layers_cnn(x2)\n",
    "  \n",
    "    x3=x2+x3\n",
    "    return x1,x2,x3\n",
    "\n",
    "  def inference(self,x):\n",
    "    # print(\"as\",x.shape)\n",
    "    x1=self.stop_linear(x)\n",
    "    x2=self.mel_linear(x)\n",
    "    x2=x2.transpose(1,2)\n",
    " \n",
    "    return x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGgEY2s_RVea"
   },
   "outputs": [],
   "source": [
    "filter_length=1024\n",
    "hop_length=256\n",
    "win_length=1024\n",
    "n_mel_channels=80\n",
    "mel_fmin=0.0\n",
    "mel_fmax=8000.0\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self,d_model,heads,max_seq_len,channels_mel,channels_stop,inference=False,kernel=5,output=512,N=6):\n",
    "    super(Model, self).__init__()\n",
    "    self.encoder=Encoder(d_model,heads,max_seq_len,inference,N=6)\n",
    "    self.decoder=Decoder(d_model,heads,inference,N=6)\n",
    "    self.postnet=Postnet(d_model,channels_mel,channels_stop,kernel,output=512)\n",
    "    self.layers_cnn=CNN_projection(channels_mel,output,kernel)\n",
    "\n",
    "  def forward(self,x,y):\n",
    "    print(x.shape)\n",
    "    y=y.float()\n",
    "    y=y.transpose(1,2)\n",
    "    y=torch.cat((torch.zeros(y.shape[0],1,80).to(device_gpu),y),dim=1)\n",
    "\n",
    "    e_output=self.encoder(x)\n",
    "    d_output=self.decoder(y,e_output)\n",
    "    outputs=self.postnet(d_output)\n",
    "    return outputs\n",
    "\n",
    "  def inference(self,input):\n",
    "    text_input=text_phonemes(MAX_LEN=10,text=input)[0]\n",
    "    x=torch.LongTensor(text_input)\n",
    "    x.squeeze(1)\n",
    "\n",
    "\n",
    "    y=torch.zeros(1,1,80)\n",
    "    e_output=self.encoder(x)\n",
    "    a=[]\n",
    "    for i in range(30):\n",
    "      d_output=self.decoder(y,e_output)\n",
    "      outputs=self.postnet.inference(d_output)\n",
    "      # print(y.shape,outputs[1].transpose(1,2).shape)\n",
    "      a.append(outputs[1])\n",
    "      # print(i,torch.sigmoid(outputs[0].transpose(1,2))[0][0][0])\n",
    "      if torch.sigmoid(outputs[0].transpose(1,2))[0][0][0]>0.6:\n",
    "        print(i)\n",
    "        break\n",
    "      y=outputs[1].transpose(1,2)\n",
    "    out=a[0]\n",
    "    for i in range(1,len(a)):\n",
    "      out=torch.cat((out,a[i]),dim=2)\n",
    "    print(out.shape)\n",
    "    outputs=self.layers_cnn(out)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlgvY9BQdwI5",
    "outputId": "9f2a8279-aaad-41cf-8981-9ca89419fd84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning:\n",
      "\n",
      "nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 30])\n"
     ]
    }
   ],
   "source": [
    "net=Model(32,8,10,80,1,inference=True,N=6)\n",
    "x=torch.rand(1,10)\n",
    "x=x.type(torch.LongTensor)\n",
    "x=x.to(device_gpu)\n",
    "i=torch.rand(1,80,16)\n",
    "i=i[:,:,:15]\n",
    "i=i.to(device_gpu)\n",
    "# net=net.to(device_gpu)\n",
    "# a=net(x,i)\n",
    "net.eval()\n",
    "a=net.inference('यतीन्द्र का सुर की बारादरी')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEGwCOiYeQW4"
   },
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "  def __init__(self,value):\n",
    "    super(Loss,self).__init__()\n",
    "    self.value=value\n",
    "  \n",
    "  def forward(self,model_outputs,targets):\n",
    "    stop_output,mel_output_final,mel_ouput = model_outputs\n",
    "    _,true_stop,true_mel = targets\n",
    "    true_stop=true_stop.to(device_gpu)\n",
    "    true_mel=true_mel.to(device_gpu)\n",
    "    true_stop.requires_grad=False\n",
    "    true_mel.requires_grad=False\n",
    "\n",
    "    mel_loss=nn.MSELoss()(mel_output,true_mel)+nn.MSELoss()(mel_output_final,true_mel)\n",
    "    stop_loss=nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([self.value]))(stop_output,true_stop)\n",
    "    loss=mel_loss+stop_loss\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Tkz6sxsueS0"
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle(\"/content/drive/MyDrive/Text-To-Speech_HINDI/text_selected.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTgb_7yLEVLT"
   },
   "outputs": [],
   "source": [
    "df_1[\"filenumber_male\"]=df_1[\"filenumber_male\"].astype(int)\n",
    "df[\"text_id\"]=df[\"text_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRBUuu84E3yz"
   },
   "outputs": [],
   "source": [
    "df_1=df_1.sort_values(by=['filenumber_male'])\n",
    "df=df.sort_values(by=['text_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2eeefj2qDSC"
   },
   "outputs": [],
   "source": [
    "padded_audio=torch.LongTensor(df_1.shape[0],80,2671)\n",
    "output_lengths = torch.FloatTensor(2671)\n",
    "padded_audio.zero_()\n",
    "for i in range(df_1.shape[0]):\n",
    "  padded_audio[i,:,:df_1[\"mel_output\"].values[i].size(1)]=df_1[\"mel_output\"].values[i]\n",
    "  output_lengths[i] = df_1[\"mel_output\"].values[i].size(1)\n",
    "\n",
    "padded_text=torch.LongTensor(df.shape[0],1,152)\n",
    "for i in range(df.shape[0]):\n",
    "  padded_text[i,:,:]=torch.Tensor(df[\"text_processed\"].values[i])\n",
    "padded_text=padded_text.squeeze(1)\n",
    "\n",
    "stop_vector=torch.LongTensor(df_1.shape[0],2671)\n",
    "stop_vector.zero_()\n",
    "for i in range(df_1.shape[0]):\n",
    "  stop_vector[i,df_1[\"mel_output\"].values[i].size(1)-1:]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWuCOMUjFtDI"
   },
   "outputs": [],
   "source": [
    "train_text,validation_text,train_audio,validation_audio=train_test_split(padded_text,padded_audio,random_state=2018, test_size=0.1)\n",
    "train_stop,validation_stop,_,_=train_test_split(stop_vector,padded_audio,random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlgIkaGyJpQg"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "\n",
    "batch_size = 64\n",
    "train_data = TensorDataset(train_text, train_stop, train_audio)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = TensorDataset(validation_text, validation_stop, validation_audio)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=validation_text.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llsLrLdUfwPz"
   },
   "outputs": [],
   "source": [
    "loss_values=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf2_3iZOgxiU"
   },
   "outputs": [],
   "source": [
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "def save_checkpoint(state, is_best, filename='/content/drive/MyDrive/Text-To-Speech_HINDI/checkpoint.pth.tar'):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Training Loss did not improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzXK9x8ag2DC"
   },
   "outputs": [],
   "source": [
    "resume_weights = '/content/drive/MyDrive/Text-To-Speech_HINDI/checkpoint.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXmEOpTbhGL-"
   },
   "outputs": [],
   "source": [
    "# If exists a best model, load its weights!\n",
    "if os.path.isfile(resume_weights):\n",
    "    #print(\"=> loading checkpoint '{}' ...\".format(resume_weights))\n",
    "    if device_gpu:\n",
    "        checkpoint = torch.load(resume_weights)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(resume_weights,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\",checkpoint['epoch'],best_accuracy,start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM_pKHE2KPhC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "best_accuracy=torch.FloatTensor([0.0005])\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "net=Model(512,8,152,2671,80,1,N=6)\n",
    "net=net.to(device_gpu)\n",
    "for epoch_i in range(0, 2):\n",
    "  for i, data in enumerate(train_dataloader):\n",
    "    text=data[0].long().to(device_gpu)\n",
    "    stop=data[1].long().to(device_gpu)\n",
    "    audio=data[2].to(device_gpu)\n",
    "    \n",
    "    net.zero_grad() \n",
    "    outputs=net(text,audio)\n",
    "    loss_model=Loss()\n",
    "    loss=loss_model(outputs,data)\n",
    "    total_loss += loss.item()/64 \n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "  avg_train_loss = total_loss / len(train_dataloader)\n",
    "  acc = torch.FloatTensor([avg_train_loss])\n",
    "  # Get bool not ByteTensor\n",
    "  is_best = bool(acc.numpy() < best_accuracy.numpy())\n",
    "  # Get greater Tensor to keep track best acc\n",
    "  best_accuracy = torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n",
    "  save_checkpoint({\n",
    "                  'epoch': start_epoch + epoch_i + 1,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'best_accuracy': best_accuracy\n",
    "              }, is_best)\n",
    "  loss_values.append(avg_train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQocwb9lTi_e"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_phonemes(MAX_LEN=153,text=\"यतीन्द्र मिश्र का सुर की बारादरी, इस नाम से, पेंग्विन यात्रा से शीघ्र प्रकाश्य पुस्तक से लिया गया है, जो उनकी कला, स्थानीय परम्परा, उनके व्यक्तित्व को, एक साथ पढ़ती हैं\"):\n",
    "  vowels=[\"ा\",\"ि\",\"ी\",\"ु\",\"ू\",\"ृ\",\"े\",\"ै\",\"ो\",\"ौ\",\"ँ\",'ं','ॆ',\"ː\",\"आ\",\"इ\",\"ई\",\"उ\",\"ऊ\",\"ऋ\",\"ए\",\"ऐ\",\"ओ\",\"औ\",\"अः\",\"अं\"]\n",
    "  full_consonants=[\"क\",\"ख\",\"ग\",\"घ\",\"ङ\",\"च\",\"छ\",\"ज\",\"झ\",\"ञ\",\"ट\",\"ठ\",\"ड\",\"ढ\",\"ण\",\"त\",\"थ\",\"द\",\"ध\",\"न\",\"प\",\"फ\",\"ब\",\"भ\",\"म\",\"य\",\"र\",\"ल\",\"ळ\",\"व\",\"श\",\"ष\",\"स\",\"ह\"]\n",
    "  special_vowels=[\"ः\",\"ं\",\"ँ\"]\n",
    "  mapping={}\n",
    "  mapping[\"अ\"]=\"ə\"\n",
    "  mapping[\"आ\"]=\"a:\"\n",
    "  mapping[\"इ\"]=\"ɪ\"\n",
    "  mapping[\"ई\"]=\"i:\"\n",
    "  mapping[\"उ\"]=\"ʊ\"\n",
    "  mapping[\"ऊ\"]=\"u:\"\n",
    "  mapping[\"ऋ\"]=\"ɻ̩\"\n",
    "  mapping[\"ए\"]=\"e:\"\n",
    "  mapping[\"ऐ\"]=\"ɛ:\"\n",
    "  mapping[\"ओ\"]=\"o:\"\n",
    "  mapping[\"औ\"]=\"ɔ:\"\n",
    "  #mapping[\"अँ\"]=\"/ə̃/\"\n",
    "  #mapping[\"अः\"]=\"/əɦə/\"\n",
    "  mapping[\"अं\"]=\"əm\"\n",
    "  #mapping[\"ऑ\"]=\"/ɒ/\"\n",
    "\n",
    "\n",
    "  mapping[\"ा\"]=\"a:\"\n",
    "  mapping[\"ि\"]=\"ɪ\"\n",
    "  mapping[\"ी\"]=\"i:\"\n",
    "  mapping[\"ु\"]=\"ʊ\"\n",
    "  mapping[\"ू\"]=\"u:\"\n",
    "  mapping[\"ृ\"]=\"ɻ̩\"\n",
    "  mapping[\"े\"]=\"e:\"\n",
    "  mapping[\"ै\"]=\"ɛ:\"\n",
    "  mapping[\"ो\"]=\"o:\"\n",
    "  mapping[\"ौ\"]=\"ɔ:\"\n",
    "  #mapping[\"ँ\"]=\"/ə̃/\"\n",
    "  #mapping[\"ː\"]=\"/əɦə/\"\n",
    "  mapping['ं']=\"əm\"\n",
    "  mapping[\"ॆ\"]=\"e:\"\n",
    "\n",
    "  mapping[\"क\"+\"्\"]=\"k\"\n",
    "  mapping[\"ख\"+\"्\"]=\"kʰ\"\n",
    "  mapping[\"ग\"+\"्\"]=\"g\"\n",
    "  mapping[\"घ\"+\"्\"]=\"gʰ\"\n",
    "  mapping[\"ङ\"+\"्\"]=\"ŋ\"\n",
    "  mapping[\"च\"+\"्\"]=\"tʃ\"\n",
    "  mapping[\"छ\"+\"्\"]=\"tʃʰ\"\n",
    "  mapping[\"ज\"+\"्\"]=\"dʒ\"\n",
    "  mapping[\"झ\"+\"्\"]=\"dʒʰ\"\n",
    "  mapping[\"ञ\"+\"्\"]=\"ɲ\"\n",
    "  mapping[\"ट\"+\"्\"]=\"ʈ\"\n",
    "  mapping[\"ठ\"+\"्\"]=\"ʈʰ\"\n",
    "  mapping[\"ड\"+\"्\"]=\"ɖ\"\n",
    "  mapping[\"ढ\"+\"्\"]=\"ɖʰ\"\n",
    "  mapping[\"ण\"+\"्\"]=\"ɳ\"\n",
    "  mapping[\"त\"+\"्\"]=\"t̪\"\n",
    "  mapping[\"थ\"+\"्\"]=\"t̪ʰ\"\n",
    "  mapping[\"द\"+\"्\"]=\"d̪\"\n",
    "  mapping[\"ध\"+\"्\"]=\"d̪ʰ\"\n",
    "  mapping[\"न\"+\"्\"]=\"n\"\n",
    "  mapping[\"प\"+\"्\"]=\"p\"\n",
    "  mapping[\"फ\"+\"्\"]=\"pʰ\"\n",
    "  mapping[\"ब\"+\"्\"]=\"b\"\n",
    "  mapping[\"भ\"+\"्\"]=\"bʰ\"\n",
    "  mapping[\"म\"+\"्\"]=\"m\"\n",
    "  mapping[\"य\"+\"्\"]=\"j\"\n",
    "  mapping[\"र\"+\"्\"]=\"ɾ\"\n",
    "  mapping[\"ल\"+\"्\"]=\"l\"\n",
    "  mapping[\"व\"+\"्\"]=\"ʋ\"\n",
    "  mapping[\"स\"+\"्\"]=\"s\"\n",
    "  mapping[\"श\"+\"्\"]=\"ʃ\"\n",
    "  mapping[\"ष\"+\"्\"]=\"ʂ\"\n",
    "  mapping[\"ह\"+\"्\"]=\"ɦ\"\n",
    "  mapping[\"ळ\"+\"्\"]=\"ɭ̆ɭ̆\"\n",
    "  mapping[\" \"]=\"SIL\"\n",
    "\n",
    "\n",
    "  #text=\"नमस्कार नमस्कार नमस्कार नमस्कार नमस्कार नमस्कार नमस्कार नमस्कार \"\n",
    "  line=text.split(\"\\n\")\n",
    "  for i in range(len(line)):\n",
    "    line[i]=line[i].strip()\n",
    "  # print(line)\n",
    "  b=[]\n",
    "\n",
    "  for k in range(len(line)):\n",
    "    x=str(b)+str(\"_\")+str(k)\n",
    "    x=[]\n",
    "    a=line[k]\n",
    "    a_1=re.split(\" \",a)\n",
    "    #print(a_1)\n",
    "    \n",
    "    for i in range(len(a_1)):\n",
    "      for j in range(len(a_1[i])):\n",
    "      \n",
    "        #print(a_1[i])\n",
    "        x.append(a_1[i][j])\n",
    "      x.append(\"  \")\n",
    "    b.append(x)\n",
    "\n",
    "  for i in range(len(b)):\n",
    "    for j in range(len(b[i])):\n",
    "      if b[i][j]==\"अ\" and b[i][j+1] in special_vowels:\n",
    "        b[i][j]=\"अ\"+b[i][j+1]\n",
    "        b[i][j+1]=\".\"\n",
    "    if \".\" in b[i]:\n",
    "      b[i].remove(\".\")\n",
    "\n",
    "  for i in range(len(b)):\n",
    "    for j in range(len(b[i])):\n",
    "      if b[i][j]==\"्\":\n",
    "        #print(b[i-1],i)\n",
    "        b[i][j-1]=b[i][j-1]+b[i][j]\n",
    "        b[i][j]=\"\"\n",
    "\n",
    "  for i in range(len(b)):\n",
    "    for j in range(3*len(b[i])):\n",
    "      if j < len(b[i]):\n",
    "        if b[i][j] in full_consonants and b[i][j+1] not in vowels:\n",
    "          b[i][j]=b[i][j]+\"्\"\n",
    "          b[i].insert(j+1,\"अ\")\n",
    "\n",
    "    if b[i][len(b[i])-2] in full_consonants:\n",
    "      b[i][len(b[i])-2]=b[i][len(b[i])-2]+\"्\"\n",
    "      b[i].insert(len(b[i])-1,\"अ\")\n",
    "\n",
    "  c=[]\n",
    "  indices=[]\n",
    "  for i in range(len(b)):\n",
    "    y=str(c)+str(\"_\")+str(i)\n",
    "    y=[]\n",
    "    for j in range(len(b[i])):\n",
    "      if b[i][j]!=\"\":\n",
    "        y.append(b[i][j])\n",
    "    c.append(y)\n",
    "\n",
    "\n",
    "  for k in range(len(c)):\n",
    "    for i in range(1,len(c[k])):\n",
    "      if c[k][i] in vowels and c[k][i-1] not in vowels:\n",
    "        c[k][i-1]=c[k][i-1]+\"्\"\n",
    "\n",
    "  phones=[]\n",
    "\n",
    "  for i in range(len(c)):\n",
    "    z=str(phones)+str(\"_\")+str(i)\n",
    "    z=[]\n",
    "    values=''\n",
    "    for char in c[i]:  \n",
    "      if char=='  ':\n",
    "        z.append(values)\n",
    "        values=''\n",
    "        \n",
    "      else:\n",
    "        for key,value in mapping.items():\n",
    "          if char == key:\n",
    "            values=values+value+\" \"\n",
    "      \n",
    "      \n",
    "    phones.append(z)\n",
    "\n",
    "  phones_final=[]\n",
    "  for i in range(len(phones)):\n",
    "    z=str(phones_final)+str(\"_\")+str(i)\n",
    "    z=[]\n",
    "    for j in range(len(phones[i])):\n",
    "      if len(phones[i][j])==0:\n",
    "        break\n",
    "      else:\n",
    "        z.append(phones[i][j])\n",
    "    \n",
    "    phones_final.append(z)\n",
    "  for j in range(len(phones_final)):\n",
    "    for i in range(1,5*len(phones_final[j]),2):\n",
    "      if i<len(phones_final[j]):\n",
    "        phones_final[j].insert(i,\"SIL\")\n",
    "  # print(phones_final)\n",
    "  # phonemes=[]\n",
    "  # for i in range(len(phones_final[0])):\n",
    "  #   for j in range(len(phones_final[0][i])):\n",
    "  #     phonemes.append(phones_final[0][i][j].split(\" \"))\n",
    "  phonemes=[]\n",
    "  temp_pho=[]\n",
    "  for j in range(len(phones_final)):\n",
    "    a=[]\n",
    "    for i in range(len(phones_final[j])):\n",
    "      a.append(phones_final[j][i])\n",
    "    temp_pho.append(a)\n",
    "  # print(temp_pho)\n",
    "  for j in range(len(temp_pho)):\n",
    "    x=[]\n",
    "    for i in range(len(temp_pho[j])):\n",
    "      temp_pho[j][i]=temp_pho[j][i].strip()\n",
    "      x.append(temp_pho[j][i].split(\" \"))\n",
    "    phonemes.append(x)\n",
    "  # print(phonemes)\n",
    "  phonemes_final=[]\n",
    "  for i in range(len(phonemes)):\n",
    "    y=[]\n",
    "    for j in range(len(phonemes[i])):\n",
    "      for k in range(len(phonemes[i][j])):\n",
    "        y.append(phonemes[i][j][k])\n",
    "    phonemes_final.append(y)\n",
    "\n",
    "\n",
    "  vocab=[]\n",
    "  for key,values in mapping.items():\n",
    "    vocab.append(values)\n",
    "  word_to_ix = {word: i+1 for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "  phonemes_num=[]\n",
    "  for j in range(len(phonemes_final)):\n",
    "    a=[]\n",
    "    for i in range(len(phonemes_final[j])):\n",
    "      # print([word_to_ix[w] for w in phonemes_final[i]])\n",
    "      a.append(word_to_ix[phonemes_final[j][i]])\n",
    "    phonemes_num.append(a)\n",
    "\n",
    "  # MAX_LEN = 100\n",
    "  #print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "  #print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "  input_ids = pad_sequences(phonemes_num, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "  return input_ids,len(phonemes_num[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text-To-Speech(Refined).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
